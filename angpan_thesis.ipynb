{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OI7CixlZnIFG",
        "E7MWmKOswQqW",
        "FEOx2jnyw9v6"
      ],
      "authorship_tag": "ABX9TyOa5rOv4T2KYdmsZRTLvnP8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aDtoiGjQWTj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03rM-QQoZ05W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, pipeline\n",
        "import openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load dataset"
      ],
      "metadata": {
        "id": "B1eExChklB1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('dataset_labeled.csv')"
      ],
      "metadata": {
        "id": "hiQP4XOvaLFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "oOMVdMQVrSGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "I16e2tcolSrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                             #remove new line symbons, replace http/https links with the word \"link\" and in case the text is just a link replace with NaN\n",
        "data['message_processed'] = data.message.map(lambda x: x.replace('\\n', '').strip())\\\n",
        "                                        .map(lambda x: ' '.join(['link' if word.startswith('http') else word for word in x.split()]))\\\n",
        "                                        .map(lambda x: None if ((len(x.split()) == 1) and (x == 'link')) else x)"
      ],
      "metadata": {
        "id": "tRlsDpbgpMDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['message_length'] = data.message_processed.map(lambda x: 0 if x is None else len(x.split()))"
      ],
      "metadata": {
        "id": "fSyIpB7WoKup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('sentiment').agg({'message_length':'mean'})"
      ],
      "metadata": {
        "id": "gjekgvh-rEc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['message_length'].describe()"
      ],
      "metadata": {
        "id": "JfYf1yahoL-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data.message_length <= 2]"
      ],
      "metadata": {
        "id": "gIrTOIGto8-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install greek_stemmer"
      ],
      "metadata": {
        "id": "47nKANECtHmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from greek_stemmer import GreekStemmer  # External library for stemming Greek words\n",
        "from spacy.lang.el import Greek\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize Greek NLP tools\n",
        "nlp = Greek()\n",
        "\n",
        "# Load Greek stopwords\n",
        "greek_stopwords = set(stopwords.words('greek'))\n",
        "greek_stopwords.remove(\"δεν\")\n",
        "\n",
        "def preprocess_greek_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess Greek text for models.\n",
        "    Steps: Lowercasing, stopword removal, punctuation removal, tokenization\n",
        "\n",
        "    Args:\n",
        "        text (str): Input Greek text.\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text as a single string.\n",
        "    \"\"\"\n",
        "    # 1. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # 3. Remove Greek stopwords\n",
        "    words = [word for word in words if word not in greek_stopwords]\n",
        "\n",
        "    # Join words back into a single string\n",
        "    return \" \".join(words)\n",
        "\n"
      ],
      "metadata": {
        "id": "32pjwEh7tA2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process text for models without tokenizers\n",
        "data['message_processed_for_nlp'] = data['message_processed'].apply(lambda x: None if x is None else preprocess_greek_text(x))"
      ],
      "metadata": {
        "id": "_OH1qdtTtag8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset='message_processed').reset_index()"
      ],
      "metadata": {
        "id": "4m_xmPakwhed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Splitting datset"
      ],
      "metadata": {
        "id": "f3YxNizul7ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "\n",
        "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['sentiment'])\n",
        "#TRAIN SET and Y\n",
        "X_train_transformer = data_train['message_processed']\n",
        "X_train_ml = data_train['message_processed_for_nlp']\n",
        "\n",
        "y_train = data_train['sentiment']\n",
        "##TEST SET\n",
        "X_test_transformer = data_test['message_processed']\n",
        "X_test_ml = data_test['message_processed_for_nlp']\n",
        "\n",
        "y_test = data_test['sentiment']"
      ],
      "metadata": {
        "id": "uaPGCEfSaFgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "OI7CixlZnIFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline: SVM"
      ],
      "metadata": {
        "id": "E7MWmKOswQqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "5lhIzwdxvIXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train_ml)\n",
        "X_test_vec = vectorizer.transform(X_test_ml)\n",
        "\n",
        "# Define parameter grid for SVM\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto'],  # Fine-tune with gamma for RBF kernel\n",
        "    'class_weight': [None, 'balanced']  # Handle imbalanced classes\n",
        "}\n",
        "\n",
        "# GridSearchCV with cross-validation\n",
        "svm_model = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "svm_model.fit(X_train_vec, y_train)"
      ],
      "metadata": {
        "id": "e1PErG9rvNMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "svm_preds = svm_model.predict(X_test_vec)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(\"Best SVM Parameters:\", svm_model.best_params_)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_preds))"
      ],
      "metadata": {
        "id": "JD3jtnf3vglc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, svm_preds))"
      ],
      "metadata": {
        "id": "KnonEb-ivjC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, svm_preds)\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vxROKxWTvqNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation Results Analysis\n",
        "cv_results = pd.DataFrame(svm_model.cv_results_)\n",
        "\n",
        "# Plot Cross-Validation Accuracy for C and Kernel\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=cv_results, x='param_C', y='mean_test_score', hue='param_kernel', marker='o')\n",
        "plt.xlabel('Regularization Parameter (C)')\n",
        "plt.ylabel('Mean CV Accuracy')\n",
        "plt.title('Cross-Validation Accuracy for SVM Hyperparameters')\n",
        "plt.legend(title='Kernel')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WIYCilMhvzlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional CV Insights: Gamma\n",
        "if 'param_gamma' in cv_results.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=cv_results, x='param_gamma', y='mean_test_score', hue='param_kernel')\n",
        "    plt.xlabel('Gamma (RBF Kernel)')\n",
        "    plt.ylabel('Mean CV Accuracy')\n",
        "    plt.title('Effect of Gamma on Cross-Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "LHvTBfH4vFXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional CV Insights: Gamma\n",
        "if 'param_gamma' in cv_results.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=cv_results, x='param_gamma', y='mean_test_score', hue='param_kernel')\n",
        "    plt.xlabel('Gamma (RBF Kernel)')\n",
        "    plt.ylabel('Mean CV Accuracy')\n",
        "    plt.title('Effect of Gamma on Cross-Validation Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1-sCpMDcp0HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1st Model: LSTM"
      ],
      "metadata": {
        "id": "FEOx2jnyw9v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Enhanced LSTM Model\n",
        "def build_advanced_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=128, input_length=120),\n",
        "        Bidirectional(LSTM(64, return_sequences=False)),  # Bidirectional LSTM for better context understanding\n",
        "        Dropout(0.3),  # Add dropout for regularization\n",
        "        Dense(32, activation='relu'),  # Fully connected layer\n",
        "        Dropout(0.3),  # Additional dropout for regularization\n",
        "        Dense(3, activation='softmax')  # Output layer for multi-class classification\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Define Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitors validation loss\n",
        "    patience=5,          # Stop after 3 epochs without improvement\n",
        "    restore_best_weights=True  # Restore the best weights\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='best_lstm_model.keras',  # Save the best model to this file\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "KxXu1WjHxBEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "1QhskPIIxY6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.get_params()"
      ],
      "metadata": {
        "id": "Fx722Duv0pwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
      ],
      "metadata": {
        "id": "kABTMt7I1Fgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.classes_"
      ],
      "metadata": {
        "id": "O_lLCpUP1PBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le_name_mapping"
      ],
      "metadata": {
        "id": "C6rqVseN1aDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string labels to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train_ml)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_ml)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_ml)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=120)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=120)"
      ],
      "metadata": {
        "id": "fkgFzcm2xVfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and Train the Model\n",
        "advanced_lstm_model = build_advanced_lstm_model()\n",
        "history = advanced_lstm_model.fit(\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    epochs=100,  # Maximum number of epochs\n",
        "    batch_size=64,  # Larger batch size for faster training\n",
        "    validation_split=0.3,\n",
        "    callbacks=[early_stopping, model_checkpoint]  # Early stopping and checkpointing\n",
        ")"
      ],
      "metadata": {
        "id": "VhXAnp3vxA02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eZiq2Zs2uUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "pf5LGFJztLvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_lstm_pred = np.argmax(advanced_lstm_model.predict(X_test_pad),axis = 1)"
      ],
      "metadata": {
        "id": "WnaBszI82zSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_lstm_pred)\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0vSaLqXg3fD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_lstm_pred))"
      ],
      "metadata": {
        "id": "WD25D-133mJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PHlSeIE2zML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Greek BERT"
      ],
      "metadata": {
        "id": "FYuHAOF4z5Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GREEKBERT\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the fine-tuned BERT model\n",
        "def build_finetuned_bert(model_name):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    # Define inputs\n",
        "    input_ids = Input(shape=(100,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = Input(shape=(100,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    # Wrap BERT in Lambda layer with explicit output shape\n",
        "    def bert_fn(inputs):\n",
        "        return bert_model.bert(\n",
        "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
        "        )[1]  # [1] extracts the pooled output\n",
        "\n",
        "    bert_output = Lambda(bert_fn, output_shape=(768,))({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
        "\n",
        "    # Add additional layers for fine-tuning\n",
        "    dense_output = Dense(128, activation=\"relu\")(bert_output)\n",
        "    final_output = Dense(3, activation=\"softmax\")(dense_output)\n",
        "\n",
        "    # Define the complete model\n",
        "    model = Model(inputs=[input_ids, attention_mask], outputs=final_output)\n",
        "\n",
        "    # Freeze BERT encoder layers (optional, depending on your use case)\n",
        "    for layer in bert_model.bert.encoder.layer:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "bert_model, bert_tokenizer = build_finetuned_bert('nlpaueb/bert-base-greek-uncased-v1')\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = bert_tokenizer(\n",
        "    list(X_train_transformer.tolist()), truncation=True, padding=True, max_length=100, return_tensors=\"tf\"\n",
        ")\n",
        "test_encodings = bert_tokenizer(\n",
        "    list(X_train_transformer.tolist()), truncation=True, padding=True, max_length=100, return_tensors=\"tf\"\n",
        ")\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitors validation loss\n",
        "    patience=5,          # Number of epochs to wait before stopping\n",
        "    restore_best_weights=True  # Restore the best weights after stopping\n",
        ")\n",
        "# Train the model\n",
        "history_bert = bert_model.fit(\n",
        "    x={\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
        "    y=y_train,\n",
        "    epochs=500,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "6_Xlnylqbhmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Extract data from the history object\n",
        "    epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')\n",
        "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Show plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dsALfpb32K3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the history object\n",
        "plot_training_history(history_bert)"
      ],
      "metadata": {
        "id": "yAzK5tZAummE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = bert_tokenizer(\n",
        "    list(X_test_transformer.tolist()), truncation=True, padding=True, max_length=100, return_tensors=\"tf\"\n",
        ")"
      ],
      "metadata": {
        "id": "TxnvowF54ULe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " bert_pred_train = np.argmax(bert_model.predict(\n",
        "    x={\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]}), axis = 1)"
      ],
      "metadata": {
        "id": "ETX2OTrc3tTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, bert_pred_train)\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hnt9FAcG5ArC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, bert_pred_train))"
      ],
      "metadata": {
        "id": "r1wGJqA55ZUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zero-shot OpenAI"
      ],
      "metadata": {
        "id": "0JB8mRjymsvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "def analyze_greek_sentiment(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of a Greek text using OpenAI's GPT model.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The Greek text to analyze.\n",
        "        api_key (str): Your OpenAI API key.\n",
        "\n",
        "    Returns:\n",
        "        str: A string with the sentiment classification (e.g., Positive, Negative, or Neutral)\n",
        "             and a brief explanation.\n",
        "    \"\"\"\n",
        "    # Set the API key for the session.\n",
        "    client = OpenAI(\n",
        "    api_key=\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # Define a system prompt to instruct the model about its role.\n",
        "    system_message = \"You are a sentiment analysis assistant specialized in Greek language text.\"\n",
        "\n",
        "    # Define the user message that includes the Greek text.\n",
        "    user_message = (\n",
        "        f\"Analyze the sentiment of the following Greek text. \"\n",
        "        f\"Classify it as Positive, Negative, or Neutral and return only the sentiment, nothing more.\\n\\n\"\n",
        "        f\"Text: {text}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Call the ChatCompletion endpoint with a low temperature for deterministic output.\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            temperature=0  # Lower temperature gives more deterministic results.\n",
        "        )\n",
        "\n",
        "        # Extract and return the sentiment analysis result.\n",
        "        result = response.choices[0].message.content\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        # In case of an error, return the error message.\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DukUvf1avH6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_predictions = X_train_transformer.map(lambda x : analyze_greek_sentiment(x) )"
      ],
      "metadata": {
        "id": "4vAdQ7Ozvpz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_pred_train = gpt_predictions.map({'Negative': 0, 'Neutral': 1, 'Positive': 2}).values"
      ],
      "metadata": {
        "id": "mTi14DHx0VgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_train, gpt_pred_train))"
      ],
      "metadata": {
        "id": "c04sHF8C2E3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_train, gpt_pred_train)\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bF_taruL14HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_predictions_test = X_test_transformer.map(lambda x : analyze_greek_sentiment(x) )"
      ],
      "metadata": {
        "id": "cFz2XBml5r-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_pred_test = gpt_predictions_test.map({'Negative': 0, 'Neutral': 1, 'Positive': 2}).values"
      ],
      "metadata": {
        "id": "n0E1ACDP522y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, gpt_pred_test))"
      ],
      "metadata": {
        "id": "e1lJraVv5-aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, gpt_pred_test)\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZeRq1N5g6C4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.__version__"
      ],
      "metadata": {
        "id": "P0LjJbPBwWbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    greek_text = \"Αυτό το προϊόν είναι εξαιρετικό και με έκανε πολύ ευχαριστημένο!\"\n",
        "    sentiment_result = analyze_greek_sentiment(greek_text)\n",
        "    print(\"Sentiment Analysis Result:\")\n",
        "    print(sentiment_result)"
      ],
      "metadata": {
        "id": "x5DxTgpCvWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RWvh6nbyiJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_result.choices[0].message.content"
      ],
      "metadata": {
        "id": "UIGPhFmyx5RP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}